import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve, confusion_matrix, auc
import matplotlib.pyplot as plt


df = pd.read_csv("/Users/floriankothbauer/Downloads/PROJEKT/kzp-2008-2020-timeseries.csv",encoding="latin1")

#data =df[df["JAHR"]==2019].copy()
data = df.copy()




label = "FiErg"
features = ["KT","Inst", "Adr",  "Ort", "Typ", "RWStatus", "Akt", "SL", "WB", "AnzStand","SA","PtageStatT","AustStatT","NeugStatT","Ops","Gebs","CMIb","CMIn",
            "pPatWAU","pPatWAK", "pPatLKP","pPatHOK","PersA","PersP","PersMT","PersT","PersAFall","PersPFall","PersMTFall","PersTFall","AnzBelA","AnzBelP (nur ab KZP2010)"]



#print(data[features].isna().sum())
#missing_percentage = data[features].isnull().mean() * 100

#print(missing_percentage[missing_percentage > 50].sort_values(ascending=False))

#Dropping Adr
data = data.drop(columns=["Adr"])
features.remove("Adr")
# features.remove("Ort")
# features.remove("KT")


num_features = data[features].select_dtypes("number").columns
cat_features = data[features].select_dtypes(exclude=["number"]).columns


#Percentage of missing values per categorical column
missing_percentage_cat = (data[cat_features].isnull().sum() / len(data[cat_features])) * 100
print("Missing percentage Categorical columns\n", missing_percentage_cat)
missing_percentage_label = (data[label].isnull().sum() / len(data[label])) * 100
print("Missing Data in Label\n", missing_percentage_label)

#Percentage of missing values per numeric column
missing_percentage_num = (data[num_features].isnull().sum() / len(data[num_features])) * 100
print("Missing percentage numeric columns\n", missing_percentage_num)

#Filling Categorical Columns

# fill_in = 'NA'
# data['SA'] = data['SA'].fillna(fill_in)
# data['SL'] = data['SL'].fillna(fill_in)
# data['WB'] = data['WB'].fillna(fill_in)

#Filling Numeric Columns


#Filling Label (Possibly change later)
data[label] = data[label].fillna(data[label].mean())

#Change Label to binary
data[label] = (data[label] > 0).astype(int)
print("Here stuff",data[label].head())
#Transforming object col into category
print(data[cat_features].dtypes)

print(data[cat_features].dtypes)

print(data[features])

#check

#Percentage of missing values per categorical column
missing_percentage_cat = (data[cat_features].isnull().sum() / len(data[cat_features])) * 100
print("Missing percentage Categorical columns\n", missing_percentage_cat)

#Percentage of missing values per numeric column
missing_percentage_num = (data[num_features].isnull().sum() / len(data[num_features])) * 100
print("Missing percentage numeric columns\n", missing_percentage_num)

#Preparation for Random Forest

#No scalingd
from sklearn.preprocessing import OneHotEncoder

data[cat_features] = data[cat_features].astype('category')

features_encoded = pd.get_dummies(data[features], columns=cat_features, drop_first=True)
cat_features = features_encoded.select_dtypes(include=['category']).columns

X = features_encoded
y = data[label]


#Data Splitting
from sklearn.model_selection import train_test_split
#X = data[features]
#y = data[label]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) 
from sklearn.compose import ColumnTransformer

from sklearn.pipeline import Pipeline
#Strati


# X_train_encoded=pd.get_dummies(X_train, columns=cat_features)
# X_test_encoded=pd.get_dummies(X_test, columns=cat_features)
# # Align the columns of the training and test sets
# X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)

# #Fill missing values in numerical columns with mean
# for i in X_train[num_features]:
#     X_train_encoded[i] = X_train_encoded[i].fillna(X_train_encoded[i].mean())

# fill = "Na"
# for i in X_train_encoded[cat_features]:
#     X_train_encoded[i] = X_train_encoded[i].fillna(fill)

#Fill missing values in numerical columns with mean in TRain
for i in X_train[num_features]:
    X_train[i] = X_train[i].fillna(X_train[i].mean())

fill = "Na"
for i in X_train[cat_features]:
    X_train[i] = X_train[i].fillna(fill)
#Fill Mising in Test

for i in X_test[num_features]:
    X_test[i] = X_test[i].fillna(X_train[i].mean())

for i in X_test[cat_features]:
    X_test[i] = X_test[i].fillna(fill)

#GPT Suggestion
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
#         ('num', 'passthrough', numerical_cols)  # Pass numeric columns unchanged
#     ]
# )

# # Combine into a pipeline with your classifier
# model = Pipeline(steps=[
#     ('preprocessor', preprocessor),
#     ('classifier', RandomForestClassifier(random_state=42))
# ])


#One Hot encode

# X_train_encoded=pd.get_dummies(X_train, columns=cat_features)
# X_test_encoded=pd.get_dummies(X_test, columns=cat_features)
# # Align the columns of the training and test sets
# X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)

#Model Fitting

from sklearn.ensemble import RandomForestClassifier

Random_Forest = RandomForestClassifier(class_weight="balanced",random_state=42)
Random_Forest.fit(X_train, y_train)

#Model Prediction
y_pred = Random_Forest.predict(X_test)
y_pred_proba = Random_Forest.predict_proba(X_test)[:, 1]


#Confusion Matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
plt.savefig('Confusion.png')

# #Model Evaluation

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
#Import f1 score
from sklearn.metrics import f1_score
f1_score = f1_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1_score:.4f}")

#Hyper Parameter Tuning

fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fp_rates, tp_rates)
plt.figure()
plt.plot(fp_rates, tp_rates, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.savefig('ROC.png')
plt.show()

#Most relevant features
importances = Random_Forest.feature_importances_
feature_names = X_train.columns
indices = np.argsort(importances)[::-1]
# Print the feature ranking
print("Feature ranking:")
lists = []
for f in range(X_train.shape[1]):
    lists.append((feature_names[indices[f]], importances[indices[f]]))

#Sort the feature importances
sorted_lists = sorted(lists, key=lambda x: x[1], reverse=True)
#List to pd.DataFrame
df_importances = pd.DataFrame(sorted_lists, columns=['Feature', 'Importance'])
# Print the DataFrame
print(df_importances.head(10))

#Try RF with UVFS
#Univariate feature selection
from sklearn.feature_selection import SelectKBest, f_classif
UVFS_Selector = SelectKBest(score_func=f_classif, k=10)
X_UVFS = UVFS_Selector.fit_transform(X_train, y_train)
X_UVFS_test = UVFS_Selector.transform(X_test)

mask = UVFS_Selector.get_support()
# Get the selected features
selected_features = X_train.columns[mask]
print("Selected features:", selected_features)


# scores = -np.log10(UVFS_Selector.pvalues_)
# scores /= scores.max()
# #Plot it
# X_indices = np.arange(X.shape[-1])
# plt.figure()
# plt.clf()
# plt.bar(X_indices - 0.05, scores, width=0.2)
# plt.title("Feature univariate score")
# plt.xlabel("Feature")
# plt.ylabel(r"Univariate score ($-Log(p_{value})$)")
# plt.xticks(X_indices, X.columns, rotation = 90)
# plt.tight_layout()
# plt.show()



#Optimal n_estimators

n_estimators = [5, 10, 50, 100, 105,106,107, 150]
best_roc_auc = 0

for stuff in n_estimators:
    rf = RandomForestClassifier(n_estimators=stuff, random_state=42)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    y_pred_proba = rf.predict_proba(X_test)[:, 1]
    fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
    auc_score = auc(fp_rates, tp_rates)
    if auc_score > best_roc_auc:
        best_roc_auc = auc_score
        best_n_estimators = stuff
    print(f"n_estimators: {stuff}, AUC: {auc_score:.4f}, Accuracy: {accuracy:.4f}")    

print(f"Best n_estimators: {best_n_estimators}, Best AUC: {best_roc_auc:.4f}, Accuracy: {accuracy:.4f}")




#AUC / ROC
fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fp_rates, tp_rates)
plt.figure()
plt.plot(fp_rates, tp_rates, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.savefig('ROC_allyears_opt.png')
#Confusion Matrix
RF_good = RandomForestClassifier(n_estimators=best_n_estimators,class_weight="balanced", random_state=42)
import seaborn as sns
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
plt.savefig('Confusion_allyears_opt.png')
# #Optimal max_depth
# max_depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
# best_roc_auc = 0
# for stuff in max_depths:
#     rf = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=stuff, random_state=42)
#     rf.fit(X_train, y_train)
#     y_pred = rf.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
#     y_pred_proba = rf.predict_proba(X_test)[:, 1]
#     fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
#     auc_score = auc(fp_rates, tp_rates)
#     if auc_score > best_roc_auc:
#         best_roc_auc = auc_score
#         best_max_depth = stuff
#     print(f"max_depth: {stuff}, AUC: {auc_score:.4f}, Accuracy: {accuracy:.4f}")
# #Best max_depth
# print(f"Best max_depth: {best_max_depth}, Best AUC: {best_roc_auc:.4f}, Accuracy: {accuracy:.4f}")

# #Optimal min_samples_split
# min_samples_splits = [2, 5, 10, 20]
# best_roc_auc = 0
# for stuff in min_samples_splits:
#     rf = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=stuff, random_state=42)
#     rf.fit(X_train, y_train)
#     y_pred = rf.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
#     y_pred_proba = rf.predict_proba(X_test)[:, 1]
#     fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
#     auc_score = auc(fp_rates, tp_rates)
#     if auc_score > best_roc_auc:
#         best_roc_auc = auc_score
#         best_min_samples_split = stuff
#     print(f"min_samples_split: {stuff}, AUC: {auc_score:.4f}, Accuracy: {accuracy:.4f}")

# print(f"Best min_samples_split: {best_min_samples_split}, Best AUC: {best_roc_auc:.4f}, Accuracy: {accuracy:.4f}")

# #Optimal min_samples_leaf
# min_samples_leafs = [1, 2, 5, 10]
# best_roc_auc = 0
# for stuff in min_samples_leafs:
#     rf = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split, min_samples_leaf=stuff, random_state=42)
#     rf.fit(X_train, y_train)
#     y_pred = rf.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
#     y_pred_proba = rf.predict_proba(X_test)[:, 1]
#     fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
#     auc_score = auc(fp_rates, tp_rates)
#     if auc_score > best_roc_auc:
#         best_roc_auc = auc_score
#         best_min_samples_leaf = stuff
#     print(f"min_samples_leaf: {stuff}, AUC: {auc_score:.4f}, Accuracy: {accuracy:.4f}")
# print(f"Best min_samples_leaf: {best_min_samples_leaf}, Best AUC: {best_roc_auc:.4f}, Accuracy: {accuracy:.4f}")

# ##Optimal model
# RF_optimal = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split, min_samples_leaf=best_min_samples_leaf, random_state=42)
# RF_optimal.fit(X_train, y_train)
# y_pred = RF_optimal.predict(X_test)
# y_pred_proba = RF_optimal.predict_proba(X_test)[:, 1]
# fp_rates, tp_rates, _ = roc_curve(y_test, y_pred_proba)
# roc_auc = auc(fp_rates, tp_rates)
# auc_score = auc(fp_rates, tp_rates)
# accuracy = accuracy_score(y_test, y_pred)
# print(f"Final AUC: {auc_score:.4f}, Accuracy: {accuracy:.4f}")
# #Confusion Matrix
# import seaborn as sns
# cm = confusion_matrix(y_test, y_pred)
# plt.figure(figsize=(10, 7))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No", "Yes"], yticklabels=["No", "Yes"])
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.title('Confusion Matrix')
# plt.savefig('Confusion_allyears_ultraopt.png')


# #Roc Curve AUC
# plt.figure()
# plt.plot(fp_rates, tp_rates, label='ROC curve (area = {:.2f})'.format(roc_auc))
# plt.plot([0, 1], [0, 1], 'r--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Receiver Operating Characteristic')
# plt.legend(loc='lower right')
# plt.savefig('ROC_allyears_ultraopt.png')

#Compare to more years

#Is best over all years




